# ğŸš€ NASA APOD ETL Pipeline - Complete Submission

**Student Name:** Zain Ul Abidin  
**Roll Number:** 22I-2738  
**Assignment:** MLOPS Assignment 3  
**Status:** âœ… **COMPLETE AND READY FOR SUBMISSION**

---

## ğŸ“š Documentation Overview

This repository contains a complete MLOps ETL pipeline with comprehensive documentation. 

**Start here based on what you need:**

| Document | Purpose |
|----------|---------|
| **[QUICK_START.md](QUICK_START.md)** | ğŸƒ **START HERE** - Run pipeline in 5 minutes |
| **[SUBMISSION_DOCUMENTATION.md](SUBMISSION_DOCUMENTATION.md)** | ğŸ“– Complete technical documentation |
| **[FINAL_SUBMISSION_CHECKLIST.md](FINAL_SUBMISSION_CHECKLIST.md)** | âœ… Comprehensive requirements checklist |

---

## âš¡ Quick Start (60 seconds)

```bash
# 1. Navigate to project
cd "c:\Users\zainy\OneDrive\Desktop\Semester-7\MLOPS\Assignment-3\a3"

# 2. Start Airflow
astro dev start

# 3. Open http://localhost:8080 in your browser

# 4. Login: admin / admin

# 5. Find DAG: "nasa_apod_etl_pipeline"

# 6. Click play button to run!
```

---

## âœ¨ What's Included

### âœ… Complete 5-Step ETL Pipeline
1. **Extract** - NASA APOD API with retry & fallback
2. **Transform** - Normalize data to standard format
3. **Load** - PostgreSQL + CSV storage
4. **Version** - DVC metadata with fallback
5. **Commit** - Git + GitHub push with PAT âœ…

### âœ… Production-Ready Features
- Error handling with graceful fallbacks
- Exponential backoff for API rate limits
- Comprehensive logging & monitoring
- Docker containerization
- PostgreSQL database with auto-init
- GitHub integration with automatic push
- DVC data versioning (with auto-fallback)

### âœ… Documentation
- Setup instructions
- Architecture diagrams
- API documentation
- Troubleshooting guide
- Testing procedures
- Verification scripts

---

## ğŸ”‘ Key Configuration

### GitHub Integration âœ…
```
User: zainulabidin776
Email: itsmezayynn@gmail.com
Repository: https://github.com/zainulabidin776/dag-airflow
Authentication: PAT Token (configured)
```

### Database
```
Type: PostgreSQL 12.6
Database: apod_db
Table: apod_data
Connection ID: postgres_apod
```

### API
```
Source: NASA APOD API
Retry Strategy: Exponential backoff (5 attempts)
Rate Limit Handling: Automatic retry with fallback
```

---

## ğŸ“‚ Project Structure

```
a3/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ nasa_apod_pipeline.py          # Main DAG (8 tasks)
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ etl_functions.py           # Extract, Transform, Load
â”‚   â”‚   â””â”€â”€ version_control.py         # DVC, Git, GitHub
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ apod_data.csv              # Versioned data
â”œâ”€â”€ docker-compose.override.yml         # Postgres config
â”œâ”€â”€ init_db.sql                         # Database init
â”œâ”€â”€ requirements.txt                    # Dependencies
â”œâ”€â”€ airflow_settings.yaml              # Connections
â”œâ”€â”€ .env                               # Environment (PAT token)
â””â”€â”€ Documentation/
    â”œâ”€â”€ QUICK_START.md                 # Start here!
    â”œâ”€â”€ SUBMISSION_DOCUMENTATION.md    # Full docs
    â””â”€â”€ FINAL_SUBMISSION_CHECKLIST.md  # Requirements
```

---

## ğŸ¯ Assignment Requirements

### âœ… Requirement 1: Extract
- [x] Fetch from NASA APOD API
- [x] Retry with exponential backoff
- [x] Handle rate limits (429)
- [x] Fallback to local CSV
- [x] Use placeholder if needed

### âœ… Requirement 2: Transform
- [x] Normalize API response
- [x] Validate required fields
- [x] Truncate long fields
- [x] Add metadata timestamps

### âœ… Requirement 3: Load
- [x] Load to PostgreSQL
- [x] Load to CSV
- [x] Create table automatically
- [x] Verify insertion

### âœ… Requirement 4: Version (DVC)
- [x] Initialize DVC
- [x] Version data with metadata
- [x] MD5 checksums
- [x] Handle incompatibilities

### âœ… Requirement 5: Commit & Push
- [x] Initialize Git
- [x] Configure GitHub identity
- [x] Add remote repository
- [x] Create commits
- [x] **Push to GitHub** âœ… NEW!

---

## ğŸ§ª Verification

### Run Full Pipeline
```bash
astro dev run dags test nasa_apod_etl_pipeline
```

### Check PostgreSQL
```bash
astro dev exec postgres psql -U airflow -d apod_db -c "SELECT COUNT(*) FROM apod_data;"
```

### Check CSV
```bash
astro dev exec webserver test -f /usr/local/airflow/include/data/apod_data.csv && echo "âœ“ CSV exists"
```

### Check GitHub Commits
```
Visit: https://github.com/zainulabidin776/dag-airflow
Look for new commits in the main/master branch
```

---

## ğŸ”’ Authentication

### GitHub PAT âœ…
- Configured in `.env`
- Enables automatic push to GitHub
- No manual authentication needed

### PostgreSQL
- User: `airflow`
- Password: `airflow`
- Pre-configured in Airflow

### Airflow UI
- User: `admin`
- Password: `admin`
- URL: `http://localhost:8080`

---

## ğŸš¨ Troubleshooting

### PostgreSQL Won't Start
```bash
astro dev restart
# Wait 30 seconds for containers to be healthy
```

### DVC Not Working
âœ… Already handled! Uses simulated metadata automatically.

### GitHub Push Failed
âœ… Check `.env` for valid GITHUB_TOKEN  
âœ… Verify network connectivity  
âœ… Check GitHub repo exists at: https://github.com/zainulabidin776/dag-airflow

### NASA API Rate Limited
âœ… Already handled! Uses CSV fallback or placeholder.

---

## ğŸ“Š Pipeline Architecture

```
NASA API
   â†“
Extract (with retry & fallback)
   â†“
Transform (normalize data)
   â†“
Load (PostgreSQL + CSV)
   â†“
Version (DVC metadata)
   â†“
Commit (Git with user identity)
   â†“
Push (GitHub with PAT) âœ…
```

---

## ğŸ“ Contact

**Student:** Zain Ul Abidin  
**Roll No:** 22I-2738  
**Email:** itsmezayynn@gmail.com  
**GitHub:** https://github.com/zainulabidin776  
**DAG Repo:** https://github.com/zainulabidin776/dag-airflow

---

## âœ… Submission Status

| Item | Status |
|------|--------|
| Requirements (5/5) | âœ… Complete |
| Documentation | âœ… Complete |
| Testing | âœ… Complete |
| GitHub Integration | âœ… Complete |
| Error Handling | âœ… Complete |
| Deployment | âœ… Ready |

**ğŸ‰ READY FOR SUBMISSION**

---

## ğŸ“– Next Steps

1. **Read:** [QUICK_START.md](QUICK_START.md) for immediate execution
2. **Learn:** [SUBMISSION_DOCUMENTATION.md](SUBMISSION_DOCUMENTATION.md) for details
3. **Verify:** [FINAL_SUBMISSION_CHECKLIST.md](FINAL_SUBMISSION_CHECKLIST.md) for requirements
4. **Run:** Execute `astro dev start` and access http://localhost:8080
5. **Monitor:** Watch the pipeline execute in real-time
6. **Verify:** Check PostgreSQL, CSV, and GitHub for results

---

**Happy DataPipelining! ğŸš€**

*Last Updated: November 14, 2025*
